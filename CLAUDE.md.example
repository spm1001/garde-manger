# garde-manger

Persistent, searchable memory across Claude sessions.

## Quick Start

```bash
# All commands need uv run (not globally installed)
uv run garde scan      # Index all conversation sources
uv run garde status    # Show database stats
uv run garde search "query"   # FTS5 search
uv run garde drill <id> --full  # View conversation
```

## Current State (Jan 2026)

**Working:**
- 6 source types indexed: Claude Code, Claude.ai, handoffs, cloud sessions, beads, local markdown
- 5300+ sources in database with full metadata
- Title extraction from `type:summary` entries, `<summary>` tags, and user messages
- Tool/skill metadata captured: tool_calls, files_touched, skills_used, git_commits
- FTS5 full-text search with type filtering
- **Multi-machine sync via Turso** (Jan 2026) — memory shared across Mac/Sprites

**Indexed sources:**
- Claude Code (local): ~1870 conversations from `~/.claude/projects/`
- Claude.ai: ~70 conversations (pre-generated summaries)
- Handoffs: ~200 session handoff files (in `~/.claude/handoffs/<encoded-path>/`)
- Cloud sessions: ~30 Claude Code for web sessions (via claude-data-sync)
- Beads: ~760 issues from project `.beads/` directories
- Local markdown: ~2365 files

**Complete:**
- Entity extraction pipeline — hybrid prompt with Haiku, backfill done (~1800 sessions)
- In-context extraction path for /close integration (extract-prompt + store-extraction)
- FTS5 indexes extraction summaries (sync-fts runs inline for new sessions via hook)

## Working Style: Marked Legato

This project has specs, phases, beads with dependencies — structure that creates "accountability gravity" toward execution. That's good for burning down implementation work.

But extraction quality, prompt design, and memory architecture need **exploration** — trying things, seeing what works, iterating without a predetermined endpoint. The structure can bias you toward closing tickets when the work actually needs space.

**Balance execution with exploration.** Power through the implementation, but pause for reflection and discovery. Don't rush to converge when mining for insights. "MOAR MINING" is sometimes the right call even when there are ready beads.

*Marked legato*: smooth connected execution, but each note still distinct. Don't blur through.

## Key Paths

| What | Where |
|------|-------|
| Runtime config | `~/.claude/memory/config.yaml` |
| Runtime glossary | `~/.claude/memory/glossary.yaml` |
| SQLite database | `~/.claude/memory/memory.db` |
| Source glossary | `./glossary.yaml` (copy to runtime location) |
| API key (extraction) | `~/.claude/memory/env` |
| Session-end hook | `~/.claude/hooks/session-end.sh` |
| Extraction logs | `~/.claude/extraction-logs/` |
| Prompt variants | `./prompts/variant_*.md` |
| Test corpus | `./test_corpus.yaml` |

## Architecture

```
Sources → Adapters → Database (SQLite/Turso) → FTS5 index → Search
              ↓                    ↓
         Metadata extraction    Turso sync (multi-machine)
```

### Turso Sync (Jan 2026)

Memory now syncs via Turso (SQLite edge database) for multi-machine access.

**How it works:**
- Local file (`~/.claude/memory/memory.db`) is an embedded replica
- Syncs to `claude-memory-yourusername.aws-eu-west-1.turso.io`
- Sync points: on connect, before search, on close
- Graceful offline fallback if Turso unreachable

**Credentials:**
- macOS: Keychain (`turso-claude-memory-url`, `turso-claude-memory-token`)
- Linux/Sprites: Environment variables

```bash
# Linux setup
export TURSO_CLAUDE_MEMORY_URL="libsql://claude-memory-yourusername.aws-region.turso.io"
export TURSO_CLAUDE_MEMORY_TOKEN="<token from Keychain or turso db tokens create>"
```

**Key files:**
- `database.py` — `DictRow` wrapper for libsql compatibility, auto-detects Turso
- `cli.py` — `migrate-turso` command for one-time migration

**Bulk operations:**
- `populate-raw-text` and `rebuild-fts` sync after each batch
- Batching ensures multi-machine consistency (no bypass mode)
- Expect ~2-5 minutes for large operations (vs seconds for local-only)
- **Local-mode bypass for migrations:** For large schema migrations (thousands of rows), use `Database(use_turso=False)` to skip sync overhead, then reconnect normally to sync at end. See Jan 2026 FTS5 migration for pattern.

**Gotchas:**
- libsql creates `-info` file alongside db for sync state
- Parser warnings suppressed via RUST_LOG at CLI entry (was cosmetic libsql quirk with FTS5 syntax)
- Don't run `garde scan` on Sprites — source files live on Mac
- Never modify `memory.db` directly with sqlite3 — use libsql via `get_database()`
- FTS5 uses token matching, not substring. "Reckitt" won't match "Reckitts" — the CLI auto-adds wildcard suffix ("Reckitt*") for better recall. Quoted phrases bypass this for exact matching.

**Token management:**
- Current token created: 2026-01-13, expires: never
- `garde status` shows token issued date and expiry
- If token expires, all machines fail silently until refreshed
- To rotate token:
  ```bash
  # Create new token (on any machine with turso CLI)
  turso db tokens create claude-memory --expiration never

  # Update on macOS
  security delete-generic-password -s turso-claude-memory-token
  security add-generic-password -s turso-claude-memory-token -w "<new-token>"

  # Update on Linux/Sprites
  export TURSO_CLAUDE_MEMORY_TOKEN="<new-token>"
  ```

**Adapters:**
- `claude_code.py` — local JSONL from `~/.claude/projects/`
- `claude_ai.py` — JSON from `~/.claude/claude-ai/cache/conversations/`
- `cloud_sessions.py` — JSON from `~/.claude/claude-ai/cache/sessions/`
- `handoffs.py` — markdown from `~/.claude/handoffs/<encoded-path>/` (project path encoded in parent dir name)

**Database schema:**
- `sources` — metadata (id, type, title, path, metadata JSON blob)
- `summaries` — indexed text content (includes denormalized `title` for FTS)
- `summaries_fts` — FTS5 virtual table with `content=summaries` (external content mode)

**FTS5 notes (Jan 2026 fix):**
- Uses external content mode (`content=summaries`) — no triggers, reads directly from summaries table
- After bulk inserts, FTS needs rebuild: `INSERT INTO summaries_fts(summaries_fts) VALUES('rebuild')`
- `upsert_summary()` auto-fetches title from sources if not provided

## Beads

Run `bd ready` for current work. Beads context written to `~/.claude/.session-context/<encoded-cwd>/beads.txt` at session start.

**Milestones:**
- Phase 1 complete (core loop, 4 adapters, FTS5)
- Phase 2 complete (extraction pipeline, hybrid prompt, backfill)

## Spec/Beads Alignment Pattern

This project uses a pattern for keeping the spec and beads in sync:

**Structure:**
- `spec.md` contains phases (conceptual groupings of work)
- Each phase maps to an **epic** in beads (type=epic)
- Epics have acceptance criteria derived from the phase's deliverables
- Tasks within phases become child issues of the epic

**Maintenance:**
1. **At phase completion:** Update spec.md to mark phase ✅ COMPLETE with summary of what was delivered
2. **At spec revision:** Create/update epics to match new phase structure
3. **At session start:** Check `bd ready` — if epics don't match spec phases, realign before working

**Why this matters:**
- Specs drift from reality during validation phases (learned the hard way)
- Beads without spec alignment becomes checkbox grinding without strategic direction
- Spec without beads alignment becomes aspirational fiction

**Alignment check questions:**
- Does each open phase have a corresponding epic?
- Does each epic's acceptance criteria match the spec's deliverables?
- Are completed phases marked in the spec?
- Are blocked phases reflected in bead dependencies?

## Pipeline: scan → backfill → sync-fts

These three commands serve different purposes:

| Command | What it does | When to run |
|---------|--------------|-------------|
| `garde scan` | Index metadata from source files | After new sessions/handoffs created |
| `garde backfill` | Run LLM extraction (builds, learnings) | After scan, to enrich with AI analysis |
| `garde sync-fts` | Update FTS with extraction content | After backfill, to make learnings searchable |

**Typical flow:**
```bash
garde scan                           # Index new sources
garde backfill --source-type handoff # Extract from handoffs
garde sync-fts                       # Make learnings searchable
```

**The FTS gap (Dec 2025):** Extractions create rich learnings/builds, but FTS only indexed summaries. `sync-fts` now flattens learnings into searchable text. If search can't find content you know exists, run `sync-fts`.

## Automation

Memory indexing is automated via hooks. Here's what fires when:

| Trigger | What Runs | Purpose |
|---------|-----------|---------|
| **Session end** (hook) | `garde process` | Index session + LLM extraction |
| **Session end** (hook) | `garde scan --source handoffs --source beads` | Index handoffs/beads written during /close |
| **Session start** (hook) | Health check | Warn if recent extractions failing |

**Key files:**
- `~/.claude/hooks/session-end.sh` — main automation
- `~/.claude/hooks/session-start.sh` — health check
- `~/.claude/extraction-logs/` — success/failure logs

**What's NOT automated:**
- Claude.ai conversations (need `claude-data-sync` + scan)
- Cloud sessions (need sync + scan)
- Local markdown (need manual scan)

**Health check (Jan 2026):** Session-start hook now warns if memory indexing is broken (failures with no successes in last 24h). This catches silent failures like broken paths or missing dependencies.

## Skills

**memory skill** (`~/.claude/skills/memory`) — primitives for search, drill, recent, status. Triggers on "search memory", "what did we learn about", "have we done this before", disorientation phrases.

**Old grounding skill:** `skill-grounding/` still exists in repo but is NOT symlinked. It was folded into `memory` skill (Dec 2025). Don't recreate — use `memory` instead.

## Gotchas

1. **Always use `uv run garde`** — not globally installed
2. **Warmup sessions filtered** — sessions with just "Warmup" are excluded
3. **Metadata is JSON blob** — query with `json_extract(metadata, '$.tool_count')`
4. **Cloud sessions need claude-data-sync** — run sync first to populate cache/sessions/
5. **API key for extraction** — `~/.claude/memory/env` must contain `export ANTHROPIC_API_KEY=...`
6. **Hook failures now detected** — session-start health check warns if extractions failing. Check `~/.claude/extraction-logs/*.FAILED.log` for details
7. **Extraction pollution** — old hook created "extraction sessions" in database, filter with `title NOT LIKE '%Entity Extraction%'`
8. **Handoff decoder knows limited paths** — `decode_parent_dir()` only recognizes `-Repos-` and `-.claude` patterns. Handoffs from other locations (e.g., `~/Documents/`) won't have project_path extracted. Add new patterns to `handoffs.py` if needed.
9. **Scan source filter** — use `garde scan --source handoffs` to rescan only one source type
10. **Glossary aliases matter** — search expands via glossary, but only `name` and `aliases` are checked, not internal keys. If "CSP" doesn't find "CS&P", add "CSP" as an alias.
11. **Orphaned handoffs** — ~99 handoffs (pre-Jan 2026) lack `session_id:` field and can't be programmatically linked to their sessions. They're still searchable by content. ~106 handoffs DO have `session_id:` embedded and could be retrofitted if needed. The naming convention changed: old style was `project-name-YYYY-MM-DD.md`, new style is `{session-id-first-8-chars}.md`. Retrofit is possible but not prioritized — content search works fine.
